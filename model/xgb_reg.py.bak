#! /usr/bin/python

from sklearn import ensemble
from sklearn.datasets import load_svmlight_file
import xgboost as xgb
import numpy as np  
import sys
import json
import math
import time

def t_now():
	return time.strftime("%Y-%m-%d %H:%M:%S",time.localtime(time.time()))

def inner_product(v1, v2):
	ss = 0.0
	l = max(len(v1), len(v2))
	for i in range(l):
		ss += v1[i] * v2[i]
	return ss

def squared_sum(v):
	ss = 0.0
	for vi in v:
		ss += vi * vi
	return ss

def vlen(v):
	return math.sqrt(squared_sum(v))

def normalize(matrix):
	# print "[%s] [INFO] in normalize ..." % t_now()
	for i in range(len(matrix)):
		l = vlen(matrix[i])
		for j in range(len(matrix[i])):
			matrix[i][j] /= l
	# print "[%s] [INFO] in normalize done" % t_now()

	return

def cos_similarity(v1, v2):
	l1 = vlen(v1)
	l2 = vlen(v2)
	if (l1 < 1e-6 or l2 < 1e-6):
		return 0.0
	else:
		return inner_product(v1, v2) / l1 / l2

def mean_cos_similarity(m1, m2, n):
	ans = 0.0
	for i in range(n):
		ans += cos_similarity(m1[i], m2[i])
	return ans / n

def cal_gradient(ys, vs, ls, n):
	#print "[%s] [INFO] in cal_gradient ..." % t_now()
	for i in range(n):
		#a = squared_sum(ys[i])
		#b = squared_sum(vs[i])
		c = inner_product(ys[i], vs[i])
		# print "[%s] [INFO] a=%f, b=%f, c=%f" % (time.strftime("%Y-%m-%d %H:%M:%S",time.localtime(time.time())), a, b, c)

		for j in range(10):
			ls[i][j] = (-1.0) * (c * vs[i][j] - ys[i][j])
			#ls[i][j] = (-1.0) * math.pow(a, -0.5) * (c * math.pow(b, -1.5) * vs[i][j] - ys[i][j] * math.pow(b, -0.5))

	#print "[%s] [INFO] in cal_gradient done" % t_now()

	return

# load libsvm files
# @return	size of dataset
def load_libsvm_files(mid, Xs, ys):
	n = 0
	for i in range(1, 11): 
		fp = "%s/data/fs/v%d_fs_%s_byweek.%s" % (env['project_pt'], i, mid, env['file_suffix']) # test
		X, y = load_svmlight_file(fp)
		Xs.append(X)
		if (0 == n):
			n = len(y)
			for yi in y:
				ys.append([yi])
		else:
			for i in range(0, n):
				ys[i].append(y[i])
		print "[%s] [INFO] load %s done." % (time.strftime("%Y-%m-%d %H:%M:%S",time.localtime(time.time())), fp)
	return n

def train_predict(train_id, test_id):
	# load libsvm files for training dataset
	Xs_train = []
	ys_train = []
	n_train = load_libsvm_files(train_id, Xs_train, ys_train)
	# load libsvm files for testing dataset
	Xs_test = []
	ys_test = []
	n_test = load_libsvm_files(test_id, Xs_test, ys_test)

	# models
	model = []

	# ans
	ans_train = []
	ans_test = []

	# generate predictions for training dataset
	ps_train = []
	for i in range(0, n_train):
		ps_train.append([0.0 for j in range(10)])

	# generate predictions for testing dataset
	ps_test = []
	for i in range(0, n_test):
		ps_test.append([0.0 for j in range(10)])

	# fit models
	for i in range(10):
		l = np.array([ys_train[j][i] for j in range(n_train)])
		clf = ensemble.GradientBoostingRegressor(**params)
		clf.fit(Xs_train[i].toarray(), l)
		print "[%s] [INFO] %d model training done" % (t_now(), i)
		preds_train = clf.staged_predict(Xs_train[i].toarray())
		ans_train.append([item for item in preds_train])
		print "[%s] [INFO] %d model predict for training data set done" % (t_now(), i)
		preds_test = clf.staged_predict(Xs_test[i].toarray())
		ans_test.append([item for item in preds_test])
		print "[%s] [INFO] %d model predict for testing data set done" % (t_now(), i)

	# predict for testing data set
	for i in range(params['n_estimators']):
		for j in range(10):
			for k in range(n_train):
				ps_train[k][j] = ans_train[j][i][k]
			for k in range(n_test):
				ps_test[k][j] = ans_test[j][i][k]
		print "%s,%d,%f,%f" % (t_now(), i + 1, mean_cos_similarity(ys_train, ps_train, n_train), mean_cos_similarity(ys_test, ps_test, n_test))

	return 0

def logregobj(preds, dtrain):
    labels = dtrain.get_label()
    
    grad =  2.0 / (labels ** 2) * preds - 2.0 / labels
    hess = 2.0 / (labels ** 2)
    return grad, hess

def load(fp, lines):
	f = open(fp)
	for line in f :
		lines.append(line.strip())

def run(train_fp, test_fp, pred_fp, key_fp):
	# X_train, y_train = load_svmlight_file(train_fp)
	# X_test, y_test = load_svmlight_file(test_fp)

	keys = []
	load(key_fp, keys)

	dtrain = xgb.DMatrix(train_fp)
	dtest = xgb.DMatrix(test_fp)

	params = {}
	with open("xgb_reg.params", 'r') as f:
		params = json.load(f)
	print "[%s] [INFO] params: %s\n" % (t_now(), str(params))

	model = xgb.train( params, dtrain, params['n_round'])

	pred = model.predict(dtest, ntree_limit=params['n_round'])

	f = open(pred_fp, 'w')
	for i in range(len(keys)):
		f.write(keys[i] + "," + str(pred[i]) + "\n")
	f.close()

	return 0

if __name__ == "__main__":
	print "[%s] [INFO] gradient boosting for regression from xgboost ..." % t_now()

	# if (3 != len(sys.argv)):
	# 	print "[%s] [ERROR]: check parameters!" % t_now()
	# 	sys.exit(1)

	data_fp = "E:/Di-tech/data/raw/season_1/training_data"
	#data_fp = "/Users/hugh_627/ICT/competition/di-tech-16/data/raw/season_1/test_set_1/"
	train_fp = data_fp + "/train_libsvm"
	test_fp = data_fp + "/test_libsvm"
	pred_fp = data_fp + "/ans/ans.csv"
	key_fp = data_fp + "/test_key"

	run(train_fp, test_fp, pred_fp, key_fp)

	print "[%s] [INFO] gradient boosting for regression from xgboost done\n" % t_now()